{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1a052e-5af0-4e85-8e28-c22587216b74",
   "metadata": {},
   "source": [
    "# SML Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d597747-b308-443d-8a28-fa5bee5e82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import turicreate as tc\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d29ee50-dd1c-4d7f-938c-16589c6d4133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imagine that a movie production company wants to use a sentiment analysis model to identify positive/negative reviews of their movies. \n",
    "# Which is worse for this use case, a false positive or a false negative, or are they equally bad? What value of β would be suitable for an Fβ score? \n",
    "# Explain your reasoning in markdown, and state a value of β."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584e781-4d71-4983-8cb5-29bba60b226d",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. I believe that both are equally bad. With a false negative (FN), the model would wrongly predict a review to be negative (with a positive label), which would wrongly balance the overall rating.\n",
    "With a false positive (FP), the model would wrongly predict a positive review (with a negative label), which would also wrongly balance the overall movie's rating.\n",
    "\n",
    "3. A suitable value for β in the Fβ score is 1. This value addresses the balance between precision and recall for this use case. This is because FN and FP are both equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18314f3-06e5-4c33-96b1-e476281380db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the explanation again if the markdown isn't visible:\n",
    "\n",
    "# Explanation:\n",
    "# 1. I believe that both are equally bad. With a false negative (FN), the model would wrongly predict a review to be negative (with a positive label), which would wrongly balance the overall rating. \n",
    "# With a false positive (FP), the model would wrongly predict a positive review (with a negative label), which would also wrongly balance the overall movie's rating.\n",
    "\n",
    "# 2. A suitable value for β in the Fβ score is 1. This value addresses the balance between precision and recall for this use case. This is because FN and FP are both equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15291f4d-fc56-4864-b623-7427c6440ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/mgalinsky/SML/Lab4/IMDB_Dataset.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/mgalinsky/SML/Lab4/IMDB_Dataset.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 3.65456 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 3.65456 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 39624 lines. Lines per second: 49460.1</pre>"
      ],
      "text/plain": [
       "Read 39624 lines. Lines per second: 49460.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/mgalinsky/SML/Lab4/IMDB_Dataset.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/mgalinsky/SML/Lab4/IMDB_Dataset.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 50000 lines in 0.919751 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 50000 lines in 0.919751 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2) Load the original dataset into an SFrame. Write a function to clean the data using the ‘re’ library, so that it is better suited for sentiment analysis. \n",
    "# Use the function to clean the data and store it in a new column of the SFrame.\n",
    "\n",
    "movies = tc.SFrame('IMDB_Dataset.csv')\n",
    "\n",
    "def clean(sample):\n",
    "    sample = re.sub('\\d', '', sample)\n",
    "    sample = re.sub('<.{1,4}>', ' ', sample)\n",
    "    sample = re.sub('[^\\w\\s]', ' ', sample)\n",
    "    sample = sample.lower()\n",
    "    return sample\n",
    "\n",
    "#clean = np.array(clean(review))\n",
    "movies['clean'] = movies['review'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15bd530-59a2-41a3-bc72-3bd75a40ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Use Turi Create text_analytics.count_words() to count the occurrences of each word in each review; store this data in a new column of the SFrame.\n",
    "\n",
    "movies['words'] = tc.text_analytics.count_words(movies['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbdd5ddb-e54e-4092-a8ad-9585a34de5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Split the data into training/validation/testing sets using 80%/10%/10% respectively.\n",
    "\n",
    "train_data, remaining_data = movies.random_split(0.8)\n",
    "val_data, test_data = remaining_data.random_split(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec0040b-d548-44a7-acba-0728869f32a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 38045</pre>"
      ],
      "text/plain": [
       "Number of examples          : 38045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 1</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 92683</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 92683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients      : 92684</pre>"
      ],
      "text/plain": [
       "Number of coefficients      : 92684"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting L-BFGS</pre>"
      ],
      "text/plain": [
       "Starting L-BFGS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Step size | Elapsed Time | Training Accuracy | Validation Accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Step size | Elapsed Time | Training Accuracy | Validation Accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 0         | 2        | 1.000000  | 1.262193     | 0.923722          | 0.849725            |</pre>"
      ],
      "text/plain": [
       "| 0         | 2        | 1.000000  | 1.262193     | 0.923722          | 0.849725            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 4        | 1.000000  | 1.564133     | 0.954396          | 0.849725            |</pre>"
      ],
      "text/plain": [
       "| 1         | 4        | 1.000000  | 1.564133     | 0.954396          | 0.849725            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 6        | 0.972300  | 1.843425     | 0.980024          | 0.886670            |</pre>"
      ],
      "text/plain": [
       "| 2         | 6        | 0.972300  | 1.843425     | 0.980024          | 0.886670            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 7        | 1.000000  | 2.034676     | 0.984439          | 0.888168            |</pre>"
      ],
      "text/plain": [
       "| 3         | 7        | 1.000000  | 2.034676     | 0.984439          | 0.888168            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 8        | 1.000000  | 2.217235     | 0.992824          | 0.895157            |</pre>"
      ],
      "text/plain": [
       "| 4         | 8        | 1.000000  | 2.217235     | 0.992824          | 0.895157            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 9         | 13       | 1.000000  | 3.190709     | 0.999842          | 0.870694            |</pre>"
      ],
      "text/plain": [
       "| 9         | 13       | 1.000000  | 3.190709     | 0.999842          | 0.870694            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset: \n",
      "\n",
      "Confusion Matrix: \n",
      " +--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   positive   |     positive    | 19923 |\n",
      "|   positive   |     negative    |  144  |\n",
      "|   negative   |     negative    | 19860 |\n",
      "|   negative   |     positive    |  121  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      "\n",
      "Accuracy: 0.9933829404714343\n",
      "Precision: 0.993963280782279\n",
      "Recall: 0.992824039467783\n",
      "F[1] Score: 0.9933933334995388\n",
      "AUC: 0.9983261194861089\n",
      "\n",
      " \n",
      "Validation Dataset: \n",
      "\n",
      "Confusion Matrix:\n",
      " +--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   positive   |     positive    |  2148 |\n",
      "|   positive   |     negative    |  327  |\n",
      "|   negative   |     negative    |  2195 |\n",
      "|   negative   |     positive    |  326  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      "\n",
      "Accuracy: 0.8692954363490792\n",
      "Precision: 0.8682295877122069\n",
      "Recall: 0.8678787878787879\n",
      "F[1] Score: 0.8680541523540108\n",
      "AUC: 0.9274929381077726\n"
     ]
    }
   ],
   "source": [
    "# 5) Use Turi Create to create a logistic classifier for sentiment analysis called model1. Find predictions using model.predict() with output_type='probability'; \n",
    "# do this for the training and validation sets and store them SArrays called train_pred_1 and val_pred_1.\n",
    "# Use functions within turicreate.evaluation to calculate and display, for the training and validation sets:\n",
    "#  · accuracies\n",
    "#  · confusion matrices\n",
    "#  · Fβ scores (using the value of β you chose above)\n",
    "#  · AUC\n",
    "\n",
    "model1 = tc.logistic_classifier.create(train_data, features=['words'], target='sentiment')\n",
    "\n",
    "train_pred_1 = model1.predict(train_data, output_type='probability')\n",
    "train_class_1 = model1.predict(train_data, output_type='class')\n",
    "\n",
    "# Training Dataset\n",
    "print('Training Dataset: \\n')\n",
    "print('Confusion Matrix: \\n', tc.evaluation.confusion_matrix(train_data['sentiment'], train_class_1))\n",
    "print('Accuracy:', tc.evaluation.accuracy(train_data['sentiment'], train_class_1))\n",
    "print('Precision:', tc.evaluation.precision(train_data['sentiment'], train_class_1))\n",
    "print('Recall:', tc.evaluation.recall(train_data['sentiment'], train_class_1))\n",
    "print('F[1] Score:', tc.evaluation.fbeta_score(train_data['sentiment'], train_class_1, beta=1))\n",
    "print('AUC:', tc.evaluation.auc(train_data['sentiment'], train_pred_1))\n",
    "\n",
    "val_pred_1 = model1.predict(val_data, output_type='probability')\n",
    "val_class_1 = model1.predict(val_data, output_type='class')\n",
    "\n",
    "# Validation Dataset\n",
    "print('\\n \\nValidation Dataset: \\n')\n",
    "print('Confusion Matrix:\\n', tc.evaluation.confusion_matrix(val_data['sentiment'], val_class_1))\n",
    "print('Accuracy:', tc.evaluation.accuracy(val_data['sentiment'], val_class_1))\n",
    "print('Precision:', tc.evaluation.precision(val_data['sentiment'], val_class_1))\n",
    "print('Recall:', tc.evaluation.recall(val_data['sentiment'], val_class_1))\n",
    "print('F[1] Score:', tc.evaluation.fbeta_score(val_data['sentiment'], val_class_1, beta=1))\n",
    "print('AUC:', tc.evaluation.auc(val_data['sentiment'], val_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3b87e1-735d-433a-8689-d4fc78e4323a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 38045</pre>"
      ],
      "text/plain": [
       "Number of examples          : 38045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 1</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 92650</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 92650"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients      : 92651</pre>"
      ],
      "text/plain": [
       "Number of coefficients      : 92651"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Accelerated Gradient (FISTA)</pre>"
      ],
      "text/plain": [
       "Starting Accelerated Gradient (FISTA)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Tuning step size. First iteration could take longer than subsequent iterations.</pre>"
      ],
      "text/plain": [
       "Tuning step size. First iteration could take longer than subsequent iterations."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Step size | Elapsed Time | Training Accuracy | Validation Accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Step size | Elapsed Time | Training Accuracy | Validation Accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 0         | 1        | 1.000000  | 0.127898     | 0.499829          | 0.524713            |</pre>"
      ],
      "text/plain": [
       "| 0         | 1        | 1.000000  | 0.127898     | 0.499829          | 0.524713            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 0.000008  | 3.504083     | 0.922250          | 0.862706            |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 0.000008  | 3.504083     | 0.922250          | 0.862706            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 3        | 0.000008  | 3.806792     | 0.964174          | 0.890664            |</pre>"
      ],
      "text/plain": [
       "| 2         | 3        | 0.000008  | 3.806792     | 0.964174          | 0.890664            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 4        | 0.000005  | 4.233318     | 0.961808          | 0.883674            |</pre>"
      ],
      "text/plain": [
       "| 3         | 4        | 0.000005  | 4.233318     | 0.961808          | 0.883674            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 5        | 0.000002  | 4.746242     | 0.976607          | 0.901648            |</pre>"
      ],
      "text/plain": [
       "| 4         | 5        | 0.000002  | 4.746242     | 0.976607          | 0.901648            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+-----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+-----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset: \n",
      "\n",
      "Confusion Matrix: \n",
      " +--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   positive   |     positive    | 19526 |\n",
      "|   positive   |     negative    |  541  |\n",
      "|   negative   |     negative    | 19435 |\n",
      "|   negative   |     positive    |  546  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      "\n",
      "Accuracy: 0.9728575709149021\n",
      "Precision: 0.9727979274611399\n",
      "Recall: 0.9730403149449345\n",
      "F[1] Score: 0.9729191061062806\n",
      "AUC: 0.9957801803874944\n",
      "\n",
      " \n",
      "Validation Dataset: \n",
      "\n",
      "Confusion Matrix:\n",
      " +--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   positive   |     positive    |  2274 |\n",
      "|   positive   |     negative    |  201  |\n",
      "|   negative   |     negative    |  2206 |\n",
      "|   negative   |     positive    |  315  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      "\n",
      "Accuracy: 0.8967173738991193\n",
      "Precision: 0.8783314020857474\n",
      "Recall: 0.9187878787878788\n",
      "F[1] Score: 0.8981042654028437\n",
      "AUC: 0.9551338053281735\n"
     ]
    }
   ],
   "source": [
    "# 6) Repeat step 5 to create model2, but this time experiment with l1_penalty and l2_penalty hyperparameters to get a better model. (You can try other \n",
    "# hyperparameters too if you wish. See the documentation.) Keep printing out the metrics each time you try a new model, so that you can compare its performance to model1.\n",
    "\n",
    "model2 = tc.logistic_classifier.create(train_data, features=['words'], target='sentiment', l1_penalty=0.01, l2_penalty=0.5)\n",
    "\n",
    "train_pred_2 = model2.predict(train_data, output_type='probability')\n",
    "train_class_2 = model2.predict(train_data, output_type='class')\n",
    "\n",
    "# Training Dataset\n",
    "print('Training Dataset: \\n')\n",
    "print('Confusion Matrix: \\n', tc.evaluation.confusion_matrix(train_data['sentiment'], train_class_2))\n",
    "print('Accuracy:', tc.evaluation.accuracy(train_data['sentiment'], train_class_2))\n",
    "print('Precision:', tc.evaluation.precision(train_data['sentiment'], train_class_2))\n",
    "print('Recall:', tc.evaluation.recall(train_data['sentiment'], train_class_2))\n",
    "print('F[1] Score:', tc.evaluation.fbeta_score(train_data['sentiment'], train_class_2, beta=1))\n",
    "print('AUC:', tc.evaluation.auc(train_data['sentiment'], train_pred_2))\n",
    "\n",
    "val_pred_2 = model2.predict(val_data, output_type='probability')\n",
    "val_class_2 = model2.predict(val_data, output_type='class')\n",
    "\n",
    "# Validation Dataset\n",
    "print('\\n \\nValidation Dataset: \\n')\n",
    "print('Confusion Matrix:\\n', tc.evaluation.confusion_matrix(val_data['sentiment'], val_class_2))\n",
    "print('Accuracy:', tc.evaluation.accuracy(val_data['sentiment'], val_class_2))\n",
    "print('Precision:', tc.evaluation.precision(val_data['sentiment'], val_class_2))\n",
    "print('Recall:', tc.evaluation.recall(val_data['sentiment'], val_class_2))\n",
    "print('F[1] Score:', tc.evaluation.fbeta_score(val_data['sentiment'], val_class_2, beta=1))\n",
    "print('AUC:', tc.evaluation.auc(val_data['sentiment'], val_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1decd10c-0a0a-4bba-96e4-f10e6dc73271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Use markdown to select which of your two models is the best (or declare a tie); justify your choice by commenting on the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd101dd-226a-4f4c-920c-fe0507c674f8",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "<pre>\n",
    "The model I chose is model2 with penalties of l1=0.01, and l2=0.5. Here is why I chose it:\n",
    "1. Evaluation metrics: Model2 maintains the highest turicreate evaluation metrics (accuracy, AUC, and F1 score) when compared to model2.\n",
    "   Percision and recall are also well-balanced, which, for this particular use case, makes it suitable since both FP and FN are equally important.\n",
    "2. Confusion matrix: The confusion matrix for model2 with l1=0.01 and l2=0.5 demonstrates fewer FN and FP compared to other configurations, and model1.\n",
    "\n",
    "To summarize, model2 with penalties of l1=0.01 and l2=0.5 achieves the best metrics and confusion matrix results, making it the best model for this use case.\n",
    "</pre>\n",
    "\n",
    "**Metrics for model1:**\n",
    "<pre>\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19851 |           |   positive   |     positive    |  2186 |\n",
    "|   positive   |     negative    |  123  |           |   positive   |     negative    |  325  |\n",
    "|   negative   |     negative    | 19845 |           |   negative   |     negative    |  2240 |\n",
    "|   negative   |     positive    |  129  |           |   negative   |     positive    |  279  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9936917993391409                       - Accuracy: 0.8799204771371769\n",
    "- Precision: 0.9935435435435436                      - Precision: 0.886815415821501\n",
    "- Recall: 0.9938419945929708                         - Recall: 0.8705694942254082\n",
    "- F1 Score: 0.9936927466586574                       - F1 Score: 0.8786173633440515\n",
    "- AUC: 0.9983207994163317                            - AUC: 0.9323156752606915\n",
    "</pre>\n",
    "\n",
    "\n",
    "**Metrics for model2 (L1=0.5, L2=0.1):**\n",
    "<pre>\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19545 |           |   positive   |     positive    |  2288 |\n",
    "|   positive   |     negative    |  429  |           |   positive   |     negative    |  223  |\n",
    "|   negative   |     negative    | 19506 |           |   negative   |     negative    |  2222 |\n",
    "|   negative   |     positive    |  468  |           |   negative   |     positive    |  297  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9775458095524181                       - Accuracy: 0.8966202783300199\n",
    "- Precision: 0.9766152001199221                      - Precision: 0.8851063829787233\n",
    "- Recall: 0.978522078702313                          - Recall: 0.9111907606531262\n",
    "- F1 Score: 0.9775677095055894                       - F1 Score: 0.8979591836734694\n",
    "- AUC: 0.9969633222698869                            - AUC: 0.9539597031497304\n",
    "</pre>\n",
    "\n",
    "**Metrics for model2 (L1=0.01, L2=0.5):**\n",
    "<pre>\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19387 |           |   positive   |     positive    |  2225 |\n",
    "|   positive   |     negative    |  587  |           |   positive   |     negative    |  286  |\n",
    "|   negative   |     negative    | 19590 |           |   negative   |     negative    |  2290 |\n",
    "|   negative   |     positive    |  384  |           |   negative   |     positive    |  229  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9756934014218483                       - Accuracy: 0.8976143141153081\n",
    "- Precision: 0.980577613676597                       - Precision: 0.9066829665851671\n",
    "- Recall: 0.9706117953339342                         - Recall: 0.8861011549183592\n",
    "- F1 Score: 0.9755692539942132                       - F1 Score: 0.8962739174219536\n",
    "- AUC: 0.9966686140265106                            - AUC: 0.9544284623638521\n",
    "</pre>\n",
    "\n",
    "\n",
    "**Metrics for model2 (L1=0.02, L2=0.02):**\n",
    "<pre>\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19382 |           |   positive   |     positive    |  2259 |\n",
    "|   positive   |     negative    |  592  |           |   positive   |     negative    |  252  |\n",
    "|   negative   |     negative    | 19494 |           |   negative   |     negative    |  2251 |\n",
    "|   negative   |     positive    |  480  |           |   negative   |     positive    |  268  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9731651146490438                       - Accuracy: 0.8966202783300199\n",
    "- Precision: 0.9758332494210049                      - Precision: 0.8939453897902652\n",
    "- Recall: 0.9703614699108841                         - Recall: 0.899641577060932\n",
    "- F1 Score: 0.973089667637313                        - F1 Score: 0.8967844382691543\n",
    "- AUC: 0.9960931751078139                            - AUC: 0.9530096950156123\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666b3c3b-546b-42be-9c5c-966c60fadedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplanation:\\n\\nThe model I chose is model2 with penalties of l1=0.01, and l2=0.5. Here is why I chose it:\\n1. Evaluation metrics: Model2 maintains the highest turicreate evaluation metrics (accuracy, AUC, and F1 score) when compared to model2.\\n   Percision and recall are also well-balanced, which, for this particular use case, makes it suitable since both FP and FN are equally important.\\n2. Confusion matrix: The confusion matrix for model2 with l1=0.01 and l2=0.5 demonstrates fewer FN and FP compared to other configurations, and model1.\\n\\nTo summarize, model2 with penalties of l1=0.01 and l2=0.5 achieves the best metrics and confusion matrix results, making it the best model for this use case.\\nMetrics for model1:\\n\\nTraining Set:                                        Validation Set:\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n| target_label | predicted_label | count |           | target_label | predicted_label | count |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n|   positive   |     positive    | 19851 |           |   positive   |     positive    |  2186 |\\n|   positive   |     negative    |  123  |           |   positive   |     negative    |  325  |\\n|   negative   |     negative    | 19845 |           |   negative   |     negative    |  2240 |\\n|   negative   |     positive    |  129  |           |   negative   |     positive    |  279  |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n\\n- Accuracy: 0.9936917993391409                       - Accuracy: 0.8799204771371769\\n- Precision: 0.9935435435435436                      - Precision: 0.886815415821501\\n- Recall: 0.9938419945929708                         - Recall: 0.8705694942254082\\n- F1 Score: 0.9936927466586574                       - F1 Score: 0.8786173633440515\\n- AUC: 0.9983207994163317                            - AUC: 0.9323156752606915\\nMetrics for model2 (L1=0.5, L2=0.1):\\n\\nTraining Set:                                        Validation Set:\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n| target_label | predicted_label | count |           | target_label | predicted_label | count |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n|   positive   |     positive    | 19545 |           |   positive   |     positive    |  2288 |\\n|   positive   |     negative    |  429  |           |   positive   |     negative    |  223  |\\n|   negative   |     negative    | 19506 |           |   negative   |     negative    |  2222 |\\n|   negative   |     positive    |  468  |           |   negative   |     positive    |  297  |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n\\n- Accuracy: 0.9775458095524181                       - Accuracy: 0.8966202783300199\\n- Precision: 0.9766152001199221                      - Precision: 0.8851063829787233\\n- Recall: 0.978522078702313                          - Recall: 0.9111907606531262\\n- F1 Score: 0.9775677095055894                       - F1 Score: 0.8979591836734694\\n- AUC: 0.9969633222698869                            - AUC: 0.9539597031497304\\nMetrics for model2 (L1=0.01, L2=0.5):\\n\\nTraining Set:                                        Validation Set:\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n| target_label | predicted_label | count |           | target_label | predicted_label | count |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n|   positive   |     positive    | 19387 |           |   positive   |     positive    |  2225 |\\n|   positive   |     negative    |  587  |           |   positive   |     negative    |  286  |\\n|   negative   |     negative    | 19590 |           |   negative   |     negative    |  2290 |\\n|   negative   |     positive    |  384  |           |   negative   |     positive    |  229  |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n\\n- Accuracy: 0.9756934014218483                       - Accuracy: 0.8976143141153081\\n- Precision: 0.980577613676597                       - Precision: 0.9066829665851671\\n- Recall: 0.9706117953339342                         - Recall: 0.8861011549183592\\n- F1 Score: 0.9755692539942132                       - F1 Score: 0.8962739174219536\\n- AUC: 0.9966686140265106                            - AUC: 0.9544284623638521\\nMetrics for model2 (L1=0.02, L2=0.02):\\n\\nTraining Set:                                        Validation Set:\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n| target_label | predicted_label | count |           | target_label | predicted_label | count |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n|   positive   |     positive    | 19382 |           |   positive   |     positive    |  2259 |\\n|   positive   |     negative    |  592  |           |   positive   |     negative    |  252  |\\n|   negative   |     negative    | 19494 |           |   negative   |     negative    |  2251 |\\n|   negative   |     positive    |  480  |           |   negative   |     positive    |  268  |\\n+--------------+-----------------+-------+           +--------------+-----------------+-------+\\n\\n- Accuracy: 0.9731651146490438                       - Accuracy: 0.8966202783300199\\n- Precision: 0.9758332494210049                      - Precision: 0.8939453897902652\\n- Recall: 0.9703614699108841                         - Recall: 0.899641577060932\\n- F1 Score: 0.973089667637313                        - F1 Score: 0.8967844382691543\\n- AUC: 0.9960931751078139                            - AUC: 0.9530096950156123\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Explanation:\n",
    "\n",
    "The model I chose is model2 with penalties of l1=0.01, and l2=0.5. Here is why I chose it:\n",
    "1. Evaluation metrics: Model2 maintains the highest turicreate evaluation metrics (accuracy, AUC, and F1 score) when compared to model2.\n",
    "   Percision and recall are also well-balanced, which, for this particular use case, makes it suitable since both FP and FN are equally important.\n",
    "2. Confusion matrix: The confusion matrix for model2 with l1=0.01 and l2=0.5 demonstrates fewer FN and FP compared to other configurations, and model1.\n",
    "\n",
    "To summarize, model2 with penalties of l1=0.01 and l2=0.5 achieves the best metrics and confusion matrix results, making it the best model for this use case.\n",
    "Metrics for model1:\n",
    "\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19851 |           |   positive   |     positive    |  2186 |\n",
    "|   positive   |     negative    |  123  |           |   positive   |     negative    |  325  |\n",
    "|   negative   |     negative    | 19845 |           |   negative   |     negative    |  2240 |\n",
    "|   negative   |     positive    |  129  |           |   negative   |     positive    |  279  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9936917993391409                       - Accuracy: 0.8799204771371769\n",
    "- Precision: 0.9935435435435436                      - Precision: 0.886815415821501\n",
    "- Recall: 0.9938419945929708                         - Recall: 0.8705694942254082\n",
    "- F1 Score: 0.9936927466586574                       - F1 Score: 0.8786173633440515\n",
    "- AUC: 0.9983207994163317                            - AUC: 0.9323156752606915\n",
    "Metrics for model2 (L1=0.5, L2=0.1):\n",
    "\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19545 |           |   positive   |     positive    |  2288 |\n",
    "|   positive   |     negative    |  429  |           |   positive   |     negative    |  223  |\n",
    "|   negative   |     negative    | 19506 |           |   negative   |     negative    |  2222 |\n",
    "|   negative   |     positive    |  468  |           |   negative   |     positive    |  297  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9775458095524181                       - Accuracy: 0.8966202783300199\n",
    "- Precision: 0.9766152001199221                      - Precision: 0.8851063829787233\n",
    "- Recall: 0.978522078702313                          - Recall: 0.9111907606531262\n",
    "- F1 Score: 0.9775677095055894                       - F1 Score: 0.8979591836734694\n",
    "- AUC: 0.9969633222698869                            - AUC: 0.9539597031497304\n",
    "Metrics for model2 (L1=0.01, L2=0.5):\n",
    "\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19387 |           |   positive   |     positive    |  2225 |\n",
    "|   positive   |     negative    |  587  |           |   positive   |     negative    |  286  |\n",
    "|   negative   |     negative    | 19590 |           |   negative   |     negative    |  2290 |\n",
    "|   negative   |     positive    |  384  |           |   negative   |     positive    |  229  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9756934014218483                       - Accuracy: 0.8976143141153081\n",
    "- Precision: 0.980577613676597                       - Precision: 0.9066829665851671\n",
    "- Recall: 0.9706117953339342                         - Recall: 0.8861011549183592\n",
    "- F1 Score: 0.9755692539942132                       - F1 Score: 0.8962739174219536\n",
    "- AUC: 0.9966686140265106                            - AUC: 0.9544284623638521\n",
    "Metrics for model2 (L1=0.02, L2=0.02):\n",
    "\n",
    "Training Set:                                        Validation Set:\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "| target_label | predicted_label | count |           | target_label | predicted_label | count |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "|   positive   |     positive    | 19382 |           |   positive   |     positive    |  2259 |\n",
    "|   positive   |     negative    |  592  |           |   positive   |     negative    |  252  |\n",
    "|   negative   |     negative    | 19494 |           |   negative   |     negative    |  2251 |\n",
    "|   negative   |     positive    |  480  |           |   negative   |     positive    |  268  |\n",
    "+--------------+-----------------+-------+           +--------------+-----------------+-------+\n",
    "\n",
    "- Accuracy: 0.9731651146490438                       - Accuracy: 0.8966202783300199\n",
    "- Precision: 0.9758332494210049                      - Precision: 0.8939453897902652\n",
    "- Recall: 0.9703614699108841                         - Recall: 0.899641577060932\n",
    "- F1 Score: 0.973089667637313                        - F1 Score: 0.8967844382691543\n",
    "- AUC: 0.9960931751078139                            - AUC: 0.9530096950156123\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6d297f-7461-4bb3-b442-ea43926b6319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset: \n",
      "\n",
      "Confusion Matrix: \n",
      " +--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |  2203 |\n",
      "|   positive   |     negative    |  214  |\n",
      "|   positive   |     positive    |  2244 |\n",
      "|   negative   |     positive    |  295  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      "\n",
      "Accuracy: 0.8972962066182405\n",
      "Precision: 0.8838125246159906\n",
      "Recall: 0.9129373474369405\n",
      "F[1] Score: 0.898138883329998\n",
      "AUC: 0.952799912835069\n",
      "\n",
      "\n",
      "ROC curve points with specificity:\n",
      " +-----------+--------------------+-----+------+------+----------------------+\n",
      "| threshold |        fpr         | tpr |  p   |  n   |     specificity      |\n",
      "+-----------+--------------------+-----+------+------+----------------------+\n",
      "|    0.0    |        1.0         | 1.0 | 2458 | 2498 |         0.0          |\n",
      "|   0.001   | 0.9955964771817454 | 1.0 | 2458 | 2498 | 0.004403522818254624 |\n",
      "|   0.002   | 0.9935948759007206 | 1.0 | 2458 | 2498 | 0.006405124099279402 |\n",
      "|   0.003   | 0.9919935948759008 | 1.0 | 2458 | 2498 | 0.008006405124099225 |\n",
      "|   0.004   | 0.9899919935948759 | 1.0 | 2458 | 2498 | 0.010008006405124115 |\n",
      "|   0.005   | 0.9883907125700561 | 1.0 | 2458 | 2498 | 0.011609287429943937 |\n",
      "|   0.006   | 0.9859887910328262 | 1.0 | 2458 | 2498 | 0.014011208967173783 |\n",
      "|   0.007   | 0.9819855884707767 | 1.0 | 2458 | 2498 | 0.01801441152922334  |\n",
      "|   0.008   | 0.9811849479583667 | 1.0 | 2458 | 2498 | 0.01881505204163325  |\n",
      "|   0.009   | 0.9791833466773419 | 1.0 | 2458 | 2498 | 0.02081665332265814  |\n",
      "+-----------+--------------------+-----+------+------+----------------------+\n",
      "[1001 rows x 6 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8) Using the test set, calculate and display:\n",
    "#  · accuracy\n",
    "#  · confusion matrix\n",
    "#  · Fβ score\n",
    "#  · AUC\n",
    "# Also, obtain an SFrame holding the ROC curve points using tc.evaluation.roc_curve(). You will need to convert the false positive rate into specificity. \n",
    "# Use matplotlib to create a labeled ROC curve.\n",
    "\n",
    "test_pred_2 = model2.predict(test_data, output_type='probability')\n",
    "test_class_2 = model2.predict(test_data, output_type='class')\n",
    "\n",
    "print('Test Dataset: \\n')\n",
    "print('Confusion Matrix: \\n', tc.evaluation.confusion_matrix(test_data['sentiment'], test_class_2))\n",
    "print('Accuracy:', tc.evaluation.accuracy(test_data['sentiment'], test_class_2))\n",
    "print('Precision:', tc.evaluation.precision(test_data['sentiment'], test_class_2))\n",
    "print('Recall:', tc.evaluation.recall(test_data['sentiment'], test_class_2))\n",
    "print('F[1] Score:', tc.evaluation.fbeta_score(test_data['sentiment'], test_class_2, beta=1))\n",
    "print('AUC:', tc.evaluation.auc(test_data['sentiment'], test_pred_2))\n",
    "\n",
    "#ROC curve points:\n",
    "roc_points = tc.evaluation.roc_curve(test_data['sentiment'], test_pred_2)\n",
    "roc_points['specificity'] = 1 - roc_points['fpr']\n",
    "print('\\n\\nROC curve points with specificity:\\n', roc_points)\n",
    "\n",
    "plt.plot(roc_points['specificity'], marker = 'o', label='ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed99785-19ed-4030-bd6b-74ba4aab1361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
