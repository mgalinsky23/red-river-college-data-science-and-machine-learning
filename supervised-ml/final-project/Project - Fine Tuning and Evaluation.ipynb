{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b438d094-b2b7-4799-a7a2-34aa85c73d1c",
   "metadata": {},
   "source": [
    "# Project -- Fine Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782d907c-6663-40cc-9e3f-1dca0e917c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37df4579-1891-4fa6-b03b-db8a740fe1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Revisit the ‘data analysis and preparation’ step.\n",
    "# Make any changes to the data preprocessing you wish to make, such as adding additional stopwords for text data, removing a column, or handling NaNs differently.\n",
    "# Alter the code so that only a train/test split is done. Save the CSV files.\n",
    "\n",
    "\n",
    "# No changes were needed - data contained only numerical values and no missing values. In addition, a train/test split was already used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10dd96c2-fa50-470c-8ede-7755ba516025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load the training and testing CSV files into four separate Pandas objects to hold labels and features for training, and labels and features for testing.\n",
    "\n",
    "# Sample data for better memory and efficiency in outputs\n",
    "train = pd.read_csv('Training.csv').sample(n=10000, random_state = 42)\n",
    "test = pd.read_csv('Testing.csv')\n",
    "\n",
    "train_label = np.array(train['Condition'])\n",
    "train_features = np.array(train.drop('Condition', axis=1))\n",
    "\n",
    "test_label = np.array(test['Condition'])\n",
    "test_features = np.array(test.drop('Condition', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f717d6-7d10-42f6-bc00-acfc585e12f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Weighted mean training score: 0.7187069978865335\n",
      "[[3651 1271]\n",
      " [1489 3589]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73      4922\n",
      "           1       0.74      0.71      0.72      5078\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.72      0.72      0.72     10000\n",
      "weighted avg       0.72      0.72      0.72     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Train and evaluate a decision tree using the GridSearchCV module from sklearn.\n",
    "# Define a dictionary of hyperparameter options you wish to try. Try both Gini and entropy as the ‘criterion’. \n",
    "# Refer to your previous work on decision trees to get an idea of which values to try for other hyperparameters.\n",
    "# Define a StratifiedKFold object to setup cross-validation.\n",
    "# Choose a metric to use for determining the best model. Define a make_scorer object that uses that metric and the ‘weighted’ average option.\n",
    "# Define a GridSearchCV object for a decision tree, using the above. (Using HavingGridSearchCV if this is too slow.)\n",
    "# Run the grid search to train a decision tree.\n",
    "# Display the best model hyperparameters, the weighted mean training score, and the weighted mean cross-validation score.\n",
    "# Find predictions on the training data and use them to display the confusion matrix and classification report.\n",
    "# Try new hyperparameters and repeat the above until you are satisfied that the best possible model has been found.\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2, average='weighted')\n",
    "\n",
    "dt_grid_search = GridSearchCV( estimator=DecisionTreeClassifier(), param_grid=param_grid, cv=stratified_kfold, scoring=f2_scorer, return_train_score=True)\n",
    "dt_grid_search.fit(train_features, train_label)\n",
    "\n",
    "print(\"Best parameters:\", dt_grid_search.best_params_)\n",
    "print(\"Weighted mean training score:\", dt_grid_search.best_score_)\n",
    "\n",
    "pred = dt_grid_search.predict(train_features)\n",
    "print(confusion_matrix(train_label, pred))\n",
    "print(classification_report(train_label, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e1a1bd-306e-4401-8f78-3dd09cdabb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'svc__C': 15, 'svc__decision_function_shape': 'ovr', 'svc__degree': 1, 'svc__gamma': 10, 'svc__kernel': 'linear'}\n",
      "Weighted mean training score: 0.7035474257492174\n",
      "[[3990  932]\n",
      " [1876 3202]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74      4922\n",
      "           1       0.77      0.63      0.70      5078\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.73      0.72      0.72     10000\n",
      "weighted avg       0.73      0.72      0.72     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Train and evaluate an SVM using the GridSearchCV module from sklearn.\n",
    "# Define a Pipeline object that uses standardization or normalization, prior to fitting an SVC estimator with class_weight='balanced'.\n",
    "# Define a dictionary of hyperparameter options you wish to try. Refer to your previous work on decision trees to get an idea of which values to try for other hyperparameters.\n",
    "# Define a StratifiedKFold object to setup cross-validation.\n",
    "# Choose a metric to use for determining the best model. Define a make_scorer object that uses that metric and the ‘weighted’ average option.\n",
    "# Define a GridSearchCV object for a decision tree, using the above. (Using HavingGridSearchCV if this is too slow.)\n",
    "# Run the grid search to train an SVM classifier.\n",
    "# Display the best model hyperparameters, the weighted mean training score, and the weighted mean cross-validation score.\n",
    "# Find predictions on the training data and use them to display the confusion matrix and classification report.\n",
    "# Try new hyperparameters and repeat the above until you are satisfied that the best possible model has been found.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "    'svc__degree': [1, 10, 15, 20, 50],\n",
    "    'svc__gamma': [1, 5, 10, 15],\n",
    "    'svc__C': [5, 10, 15, 30, 50],\n",
    "    'svc__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2, average='weighted')\n",
    "\n",
    "svm_grid_search = HalvingGridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring=f2_scorer, return_train_score=True)\n",
    "svm_grid_search.fit(train_features, train_label)\n",
    "\n",
    "print(\"Best parameters:\", svm_grid_search.best_params_)\n",
    "print(\"Weighted mean training score:\", svm_grid_search.best_score_)\n",
    "\n",
    "pred = svm_grid_search.predict(train_features)\n",
    "print(confusion_matrix(train_label, pred))\n",
    "print(classification_report(train_label, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6d2660-f30b-4bb6-bc19-98eb09135559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Add a markdown cell to compare the best decision tree and SVM models. Discuss overfitting/underfitting, how well the models do on important metrics, \n",
    "# and any other observations you feel are important. Decide which model is best and save it to a file using the ‘pickle’ library.\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm_grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44491c74-92d7-4a43-abb1-793f1586d280",
   "metadata": {},
   "source": [
    "# **Explanation:**\n",
    "\n",
    "# Decision Tree Model\n",
    "**1st Attempt:**\n",
    "<pre>\n",
    "Best parameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
    "Weighted mean training score: 0.7182680438484256\n",
    "[[3939  983]\n",
    " [1727 3351]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.70      0.80      0.74      4922\n",
    "           1       0.77      0.66      0.71      5078\n",
    "\n",
    "    accuracy                           0.73     10000\n",
    "   macro avg       0.73      0.73      0.73     10000\n",
    "weighted avg       0.73      0.73      0.73     10000\n",
    "</pre>\n",
    "\n",
    "**2nd and Final Attempt:**\n",
    "<pre>\n",
    "Best parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 5, 'min_samples_split': 6}\n",
    "Weighted mean training score: 0.7187069978865335\n",
    "[[3651 1271]\n",
    " [1489 3589]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.74      0.73      4922\n",
    "           1       0.74      0.71      0.72      5078\n",
    "\n",
    "    accuracy                           0.72     10000\n",
    "   macro avg       0.72      0.72      0.72     10000\n",
    "weighted avg       0.72      0.72      0.72     10000\n",
    "</pre>\n",
    "\n",
    "# SVM Model\n",
    "**1st and Final Attempt:**\n",
    "<pre>\n",
    "Best parameters: {'svc__C': 20, 'svc__decision_function_shape': 'ovo', 'svc__degree': 10, 'svc__gamma': 3, 'svc__kernel': 'linear'}\n",
    "Weighted mean training score: 0.7147642016470885\n",
    "[[3990  932]\n",
    " [1876 3202]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.81      0.74      4922\n",
    "           1       0.77      0.63      0.70      5078\n",
    "\n",
    "    accuracy                           0.72     10000\n",
    "   macro avg       0.73      0.72      0.72     10000\n",
    "weighted avg       0.73      0.72      0.72     10000\n",
    "</pre>\n",
    "\n",
    "After reviewing these results, I believe that the best model is the SVM model. Based solely on the confusion matrix, this model achieves the best results when compared to the others. With this model, we can see that the False Negative values (being the worst use-case for this project) are the lowest of all, with 932 FN and 1876 FP. Moving on, there is only a marginal difference in the F2-Score, with the hundreds place digit being lower by 0.004 than the rest; this might indicate slight overfitting of the decision tree models. This quality may not seem great, but I believe that the most important attribute is the count of FNs, which this model achieves the lowest amount of. So, while the best score of all the models may not be the SVM model, this characteristic does not hold the most importance. Next, we have the classification reports. At first glance, I can see that the only values obtained for accuracy are 0.73 and 0.72, so this model does achieve a relatively good accuracy rate of 0.72. Then, we have the negative vs. the positive class results. These results show us that the model performs better in the negative than in the positive class (with 68% and 81% vs. 77% and 63%), and while this is not a great outcome for this model, the values are relatively spread out, which indicates that there is some balance between classes 0 and 1.\n",
    "\n",
    "Overall, I believe that the best model is the SVM model, as it obtains the lowest number of FNs (this being the most important attribute in this use-case), a relatively good balance between classes 0 and 1, and a reasonably well accuracy and F2-Score rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe38df5-3dbf-469b-83ba-ca0730c8afda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1769  388]\n",
      " [ 849 1369]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.82      0.74      2157\n",
      "           1       0.78      0.62      0.69      2218\n",
      "\n",
      "    accuracy                           0.72      4375\n",
      "   macro avg       0.73      0.72      0.71      4375\n",
      "weighted avg       0.73      0.72      0.71      4375\n",
      "\n",
      "0.6439928497506822\n"
     ]
    }
   ],
   "source": [
    "# 6) Evaluate and discuss the final model.\n",
    "# Load the model from its pickle file.\n",
    "# Use the model to make predictions on the test data.\n",
    "# Use the predictions to display the confusion matrix, classification report, and any other important metrics not already in the report.\n",
    "# Add a markdown cell to discuss how well your model would perform in the desired use case. Be specific, quoting test metrics, when discussing how reliable you expect the predictions to be. \n",
    "# Discuss any problems and limitations of the model.\n",
    "\n",
    "# Load best model:\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "\n",
    "# Make predictions and display metrics:\n",
    "preds = best_model.predict(test_features)\n",
    "print(confusion_matrix(test_label, preds))\n",
    "print(classification_report(test_label, preds))\n",
    "print(fbeta_score(test_label, preds, beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07d847-617f-4eb6-ba46-1ff6a6da07c4",
   "metadata": {},
   "source": [
    "# **Explanation:**\n",
    "<pre>\n",
    "FN: 388 - 0.08868 | 932 - 0.0932\n",
    "FP: 849 - 0.19406 | 1876 - 0.1876\n",
    "</pre>\n",
    "**SVM Training Metrics:**\n",
    "<pre>\n",
    "Best parameters: {'svc__C': 20, 'svc__decision_function_shape': 'ovo', 'svc__degree': 10, 'svc__gamma': 3, 'svc__kernel': 'linear'}\n",
    "Weighted mean training score: 0.7147642016470885\n",
    "[[3990  932]\n",
    " [1876 3202]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.81      0.74      4922\n",
    "           1       0.77      0.63      0.70      5078\n",
    "\n",
    "    accuracy                           0.72     10000\n",
    "   macro avg       0.73      0.72      0.72     10000\n",
    "weighted avg       0.73      0.72      0.72     10000\n",
    "</pre>\n",
    "Given the previous and new metrics obtained from the testing data, this model performs reasonably well. With the FNs and FPs calculated (this being the most important quality), this model performs somewhat better than the training model. With this model's performance 1% better for FN count, and 1% worse for FP count, this model's performance is better as FNs are the worst metric for this use case. Other than the smaller differences in the confusion matrices, this model obtains somewhat of the same results for the other metrics. In conclusion, the marginal differences in the confusion matrices indicate consistent behavior between training and testing, suggesting no overfitting or underfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ee3822-9dc5-4398-b6d1-650c4d966751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "\n",
    "# FN: 388 - 0.08868 | 932 - 0.0932\n",
    "# FP: 849 - 0.19406 | 1876 - 0.1876\n",
    "\n",
    "# SVM Training Metrics:\n",
    "# Best parameters: {'svc__C': 20, 'svc__decision_function_shape': 'ovo', 'svc__degree': 10, 'svc__gamma': 3, 'svc__kernel': 'linear'}\n",
    "# Weighted mean training score: 0.7147642016470885\n",
    "# [[3990  932]\n",
    "#  [1876 3202]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.68      0.81      0.74      4922\n",
    "#            1       0.77      0.63      0.70      5078\n",
    "\n",
    "#     accuracy                           0.72     10000\n",
    "#    macro avg       0.73      0.72      0.72     10000\n",
    "# weighted avg       0.73      0.72      0.72     10000\n",
    "\n",
    "# Given the previous and new metrics obtained from the testing data, this model performs reasonably well. With the FNs and FPs calculated (this being the most important quality), \n",
    "# this model performs somewhat better than the training model. With this model's performance 1% better for FN count, and 1% worse for FP count, this model's performance is better \n",
    "# as FNs are the worst metric for this use case. Other than the smaller differences in the confusion matrices, this model obtains somewhat of the same results for the other metrics. \n",
    "# In conclusion, the marginal differences in the confusion matrices indicate consistent behavior between training and testing, suggesting no overfitting or underfitting issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
